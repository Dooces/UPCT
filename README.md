Premise (Needâ€“Cost arbitration law)

An adaptive controller allocates modelâ€‘based (MB) vs modelâ€‘free (MF) control by a graded function of controller adequacy (Need) minus control price (Cost).
Engagement of MB must increase strictly with Need and decrease strictly with Cost.
Formal statement

Need_t: a scalar, monotone measure of MBâ€™s adequacy advantage over MF at time t.
Valid instantiations (either class is acceptable):
Reliability advantage: Need_t = log Ï€_MB,t âˆ’ log Ï€_MF,t (relative precision of controllers).
Value advantage: Need_t = (EV_MB,t âˆ’ EV_MF,t) + VOI_t (expected value difference plus value of information).
Cost_t: a scalar, monotone measure of the current price of control (time/effort/energy/opportunity).
Valid instantiations:
Opportunity cost: Îº_t (average reward rate / time pressure).
Physiological cost: c_phys(S_t), strictly decreasing in surplus/alertness S_t.
Linear or affine combinations are permitted: Cost_t = k0 + k_ARRÂ·Îº_t + k_physÂ·c_phys(S_t).
Arbitration (graded, bounded)

w_MB(t) = Ïƒ(Î¸_need Â· z(Need_t) âˆ’ Î¸_cost Â· z(Cost_t) âˆ’ Î¸0)
Ïƒ is the logistic; z(Â·) denotes zâ€‘scoring for identifiability.
Î¸_need > 0 and Î¸_cost > 0 are required (strict monotonicity).
Execution and learning (standard)

Action selection: Q_mix = (1 âˆ’ w_MB) Q_MF + w_MB Q_MB; Ï€(a|s) âˆ exp(Q_mix/Ï„)
Online patch (MBâ†’MF): Q_MF â† Q_MF + Î·_patch_eff Â· (Q_MB âˆ’ Q_MF) (downâ€‘weight when MB is epistemically uncertain)
Offline distillation: minimize KL(Ï€_MB || Ï€_MF) over prioritized states
Nonâ€‘negotiable monotonic predictions (falsifiable)

Under validated Cost clamps, increasing Need must increase w_MB (âˆ‚w_MB/âˆ‚Need > 0).
Under validated Need clamps, increasing Cost must decrease w_MB (âˆ‚w_MB/âˆ‚Cost < 0).
The structured Needâ€“Cost model must outperform MFâ€‘only, MBâ€‘only, and a flexible freeâ€‘w mixture in outâ€‘ofâ€‘sample prediction.
What is variable (but not vague)

You may choose a reliabilityâ€‘based or value+VOIâ€‘based Need, provided it is a valid monotone adequacy measure.
You may choose Îº_t, c_phys(S_t), or their affine combination for Cost, provided it is a valid monotone controlâ€‘price measure.
These are â€œslots.â€ Swapping a valid Need or Cost instantiation does not change the law; violating the monotonic relations does.
Oneâ€‘line summary

Modelâ€‘based engagement is the logistic of Need minus Cost: w_MB = Ïƒ(Need âˆ’ Cost). Need is the controllerâ€™s adequacy advantage; Cost is the current price of control. Engagement must rise with Need and fall with Cost. If either monotonicity fails under proper clamps, the premise is false.


State/action values and policies

Q_MF(s,a): habit/AP value (DLS)
Q_MB(s,a): modelâ€‘based value (PFCâ€“HPC)
Q_mix(s_t,a) = (1 âˆ’ w_MB(t))Â·Q_MF(s_t,a) + w_MB(t)Â·Q_MB(s_t,a)
Ï€(a|s_t) âˆ exp(Q_mix(s_t,a)/Ï„)
ACC monitoring (what actually happened)

V_mix(s) = âˆ‘_a Ï€(a|s)Â·Q_mix(s,a)
Î´_ACC,t = |r_t + Î³Â·V_mix(s_{t+1}) âˆ’ V_mix(s_t)| (unsigned executedâ€‘policy error)
u_t = H_MF(s_t) = âˆ’âˆ‘_a Ï€_MF(a|s_t) ln Ï€_MF(a|s_t) (conflict proxy)
Optional epistemic term: u_t += Î»Â·Tr Î£_Q(s_t) (acrossâ€‘ensemble variance)
Opportunity cost and physiology

Îº update: Îº_t = (1âˆ’Ï)Â·Îº_{tâˆ’1} + ÏÂ·(r_t/Î”t)
c_phys(S_t) = c0Â·(1 âˆ’ S_t)^Î³, Î³ â‰¥ 1; then z(c_phys) for scaling
Arbitration (graded engagement via dACC)

x_t = [ z(Î´_ACC,t), z(u_t), âˆ’z(Îº_t), âˆ’z(c_phys(S_t)), 1 ]
w_MB(t) = Ïƒ(Î¸áµ€ x_t) (Î² fixed to 1)
Planning budget (if you need a concrete planner)

B_plan(t) = B_max Â· w_MB(t) (budget)
Optional depth: d(t) = ceil(d_max Â· w_MB(t))
Learning

MF learning (online RL; keep your original update for DLS):
Î´_MF,t = r_t + Î³Â·max_a Q_MF(s_{t+1},a) âˆ’ Q_MF(s_t,a_t)
Q_MF(s_t,a_t) â† Q_MF(s_t,a_t) + Î·_RL Â· Î´_MF,t
Patch (MBâ†’MF, confidenceâ€‘gated):
conf_MB(s,a) â‰ˆ 1 / (1 + Tr Î£_Q(s,a)) (proxy; higher = more confident)
Î·_patch_eff = Î·_patch Â· Ïƒ(Î±Â·conf_MB(s,a))
Q_MF(s,a) â† Q_MF(s,a) + Î·_patch_eff Â· (Q_MB(s,a) âˆ’ Q_MF(s,a))
Distill (offline, trustâ€‘region optional):
minimize L_distill = E_{sâˆˆC} [ KL(Ï€_MB(Â·|s) || Ï€_MF(Â·|s)) ]
(prioritize C by high w_MB or large |Q_MB âˆ’ Q_MF|)
Edge cases

Terminal: Î´_ACC,t = |r_t âˆ’ V_mix(s_t)|; MF terminal target Î´_MF,t = r_t âˆ’ Q_MF(s_t,a_t)
If B_plan(t) = 0: use Ï€_MF and skip Q_MB calls to avoid undefined queries.
Empirical predictions (falsifiable)

â†‘Îº_t (time pressure/high average reward): â†“w_MB, faster RTs, MF bias; dACC/STN activity attenuates.
â†‘S_t (rested/alert): â†“c_phys â†’ â†‘w_MB; more PFCâ€“HPC engagement/exploration.
â†‘Î´_ACC or â†‘H_MF: â†‘w_MB; transient dACC/STN signatures; slower, more variable choices; planning budget rises.
Perturbations: dACC disruption flattens the w_MB vs (Dâˆ’K) slope; STN DBS lowers the effective threshold; DLS disruption increases baseline w_MB.
Where it could be wrong (so you can adjust)

If Î´_ACC and H_MF donâ€™t predict engagement, swap in your labâ€™s conflict/surprise metrics (same form).
If Îº_t and c_phys donâ€™t modulate in the predicted directions, reâ€‘fit K_t with the terms that do (keep w_MB = Ïƒ(Î¸áµ€x)).
If mixture behaves poorly in your task, switch to the hard gate (w_MB > Ï„_gate) without changing the monitoring/cost pieces.

Define per skill k

Learning curve (improvability): b_k(n) = expected per-use performance benefit at competence n (e.g., error reduction, reward gain). Easy skills have fast learning rate Î±_k; hard skills have slow Î±_k and/or small asymptote b_k*.
Compute saving curve: c_k(n) = per-use planning/compute saved at competence n (how much less MA youâ€™ll need). Some skills proceduralize well (big c_k*), others not.
Practice unit costs: E_k^train (energy), T_k^train (time). These differ by skill (some practice is cheap/fast, others costly/slow).
Usage forecast: f_k(Ï„) = expected discounted future frequency of use (how often youâ€™ll need it), horizon Ï.
Prices (shadow costs): Î»E (energy), Îº (opportunity cost of time). These scalarize â€œapples and orangesâ€ into one currency.
Value and cost of improvement

Marginal value of one extra practice unit at competence n:
MV_k(n) = âˆ« e^(-ÏÏ„) f_k(Ï„) [b_kâ€²(n) + Î· c_kâ€²(n)] dÏ„
b_kâ€²(n), c_kâ€²(n): marginal gains from one unit more practice (steeper = easier to improve).
Î· â‰¥ 0 converts compute saved into the same currency as performance benefit.
Marginal cost of one practice unit:
MC_k = Î»EÂ·E_k^train + ÎºÂ·T_k^train
Decision rule (single currency ROI)

Marginal ROI:
ROI_k(n) = MV_k(n) / MC_k
Allocate practice to the skill with the highest ROI_k(n) while ROI_k(n) > 1 (benefit exceeds priced cost). Equimarginal stopping: at optimum, ROI equalizes across practiced skills.
Intuition (covers your cases)

Easy + big payoff: Î±_k high, b_k* large â†’ b_kâ€²(n) large early; if E_k^train/T_k^train modest, ROI_k Â» 1 â†’ practice now.
Easy + low payoff: b_k* small or f_k low â†’ MV_k small â†’ ROI_k may fall below 1 â†’ donâ€™t invest (or invest a little if compute saving c_k is big).
Hard + low payoff: Î±_k low, b_k* small, cost high â†’ ROI_k â‰ª 1 â†’ donâ€™t invest.
Hard + big payoff but frequent: MV_k can still beat cost if f_k is large and horizon long (low Ï). Patience (5â€‘HT) and forecasted frequency matter.
Compact closed form (common in practice)

If b_k(n) â‰ˆ b_k* (1 âˆ’ e^(âˆ’Î±_k n)) and c_k(n) â‰ˆ c_k* (1 âˆ’ e^(âˆ’Î±â€²_k n)), and define discounted usage F_k = âˆ« e^(âˆ’ÏÏ„) f_k(Ï„) dÏ„, then
MV_k(n) â‰ˆ F_k [Î±_k (b_k* âˆ’ b_k(n)) + Î· Î±â€²_k (c_k* âˆ’ c_k(n))]
MC_k = Î»EÂ·E_k^train + ÎºÂ·T_k^train
ROI_k(n) = MV_k(n) / MC_k
Neural readout (same currency, different circuits)

MV_k(n): estimated in ACC/BA10 (expected value of control over horizon) with confidence from AChâ€‘weighted precision; usage forecasts from frontoparietal/hippocampal schemas.
MC_k: priced by hypothalamus/insula/orexin/adenosine (Î»E) and average reward/DA/ACC (Îº).
Plasticity: striatum/cerebellum update AP skills; hippocampusâ†”PFC updates models/strategies; DA/NE gate learning rates (affect Î±_k).
Predictions (falsifiable)

Frequency/stability shifts: Increasing F_k (more frequent use) or lowering task volatility raises ROI_k and practice allocationâ€”independent of raw difficulty.
Price sensitivity: Raising Îº (higher average reward rate/urgency) favors skills that save time per future use (large c_k*), even if b_k* is modest. Raising Î»E suppresses practice of energyâ€‘intensive skills first.
Learningâ€‘curve sensitivity: Tasks with steeper Î±_k get early practice; as b_k(n) approaches b_k* (gap_k shrinks), practice shifts to other skills (equimarginal).
Transfer: If a skillâ€™s improvement transfers (W_{kj} > 0 to other tasks j), effective F_k increases and ROI_k risesâ€”more practice allocated.
Bottom line

Yes: â€œwildly differentâ€ improvement profiles reduce to one effective currency by pricing time and energy (Îº, Î»E), weighting by discounted future use (F_k), and multiplying by confidence/precision. The arbiter invests where marginal ROI is highest. This was implicit; the equations above make it explicit.

1) Learning curves and marginal gain

Model a generic saturating learning curve (pick any smooth mono-increasing form):

ğ‘‘
ğ‘‰
ğ‘˜
ğ‘‘
ğœ
=
ğœ‚
ğ‘˜
(
ğ‘‰
ğ‘˜
max
â¡
âˆ’
ğ‘‰
ğ‘˜
)
â‡’
ğ‘‰
ğ‘˜
(
ğœ
)
=
ğ‘‰
ğ‘˜
max
â¡
âˆ’
(
ğ‘‰
ğ‘˜
max
â¡
âˆ’
ğ‘‰
ğ‘˜
(
0
)
)
ğ‘’
âˆ’
ğœ‚
ğ‘˜
ğœ
.
dÏ„
dV
k
	â€‹

	â€‹

=Î·
k
	â€‹

(V
k
max
	â€‹

âˆ’V
k
	â€‹

)â‡’V
k
	â€‹

(Ï„)=V
k
max
	â€‹

âˆ’(V
k
max
	â€‹

âˆ’V
k
	â€‹

(0))e
âˆ’Î·
k
	â€‹

Ï„
.

The instant marginal benefit of practicing now (for a tiny 
Î”
ğœ
Î”Ï„) is:

Î”
ğ‘‰
ğ‘˜
âŸ
futureÂ valueÂ gain
â‰ˆ
ğœ‚
ğ‘˜
(
ğ‘‰
ğ‘˜
max
â¡
âˆ’
ğ‘‰
ğ‘˜
)
Î”
ğœ
.
futureÂ valueÂ gain
Î”V
k
	â€‹

	â€‹

	â€‹

â‰ˆÎ·
k
	â€‹

(V
k
max
	â€‹

âˆ’V
k
	â€‹

)Î”Ï„.
2) Intertemporal efficiency of practice vs use

Let 
ğ›¾
âˆˆ
(
0
,
1
)
Î³âˆˆ(0,1) be discounting (can depend on urgency/resource state). Then

Practice option (invest in improvement):

ğ‘ˆ
ğ‘˜
prac
(
ğ‘¡
)
=
âˆ‘
ğœ
â‰¥
1
ğ›¾
ğœ
â€‰
Î”
ğ‘‰
ğ‘˜
(
ğœ
)
ğ‘
ğ‘˜
prac
â€…â€Š
â‰ˆ
â€…â€Š
ğ›¾
â€‰
ğœ‚
ğ‘˜
(
ğ‘‰
ğ‘˜
max
â¡
âˆ’
ğ‘‰
ğ‘˜
)
ğ‘
ğ‘˜
prac
(myopicÂ one-stepÂ index)
.
U
k
prac
	â€‹

(t)=
c
k
prac
	â€‹

Ï„â‰¥1
âˆ‘
	â€‹

Î³
Ï„
Î”V
k
	â€‹

(Ï„)
	â€‹

â‰ˆ
c
k
prac
	â€‹

Î³Î·
k
	â€‹

(V
k
max
	â€‹

âˆ’V
k
	â€‹

)
	â€‹

(myopicÂ one-stepÂ index).

Use option (exploit current skill):

ğ‘ˆ
ğ‘˜
use
(
ğ‘¡
)
=
ğ‘‰
ğ‘˜
(
ğ‘¡
)
ğ‘
ğ‘˜
use
.
U
k
use
	â€‹

(t)=
c
k
use
	â€‹

V
k
	â€‹

(t)
	â€‹

.

Both are in the same currency: discounted value gain per unit cost.

3) Policy: choose what to do now

At each decision point, for each skill 
ğ‘˜
k:

Compute practice index 
ğ¼
ğ‘˜
prac
=
ğ›¾
â€‰
ğœ‚
ğ‘˜
(
ğ‘‰
ğ‘˜
max
â¡
âˆ’
ğ‘‰
ğ‘˜
)
ğ‘
ğ‘˜
prac
I
k
prac
	â€‹

=
c
k
prac
	â€‹

Î³Î·
k
	â€‹

(V
k
max
	â€‹

âˆ’V
k
	â€‹

)
	â€‹

.

Compute use index 
ğ¼
ğ‘˜
use
=
ğ‘‰
ğ‘˜
ğ‘
ğ‘˜
use
I
k
use
	â€‹

=
c
k
use
	â€‹

V
k
	â€‹

	â€‹

.

Then pick the option with the highest index across all 
{
ğ‘˜
,
mode
âˆˆ
{
prac
,
use
}
}
{k,modeâˆˆ{prac,use}}.
(If you want exploration of uncertain 
ğ‘‰
ğ‘˜
max
â¡
,
ğœ‚
ğ‘˜
V
k
max
	â€‹

,Î·
k
	â€‹

, add a posterior-uncertainty bonus â€” a bandit/Gittins-style index â€” but the currency stays identical.)

4) How this captures your three â€œwildly differentâ€ cases

Easy to improve & big payoff: large 
ğœ‚
ğ‘˜
Î·
k
	â€‹

 and large gap 
(
ğ‘‰
ğ‘˜
max
â¡
âˆ’
ğ‘‰
ğ‘˜
)
(V
k
max
	â€‹

âˆ’V
k
	â€‹

) â‡’ huge 
ğ¼
ğ‘˜
prac
I
k
prac
	â€‹

 â‡’ practice now.

Easy to improve & low payoff: large 
ğœ‚
ğ‘˜
Î·
k
	â€‹

 but small 
(
ğ‘‰
ğ‘˜
max
â¡
âˆ’
ğ‘‰
ğ‘˜
)
(V
k
max
	â€‹

âˆ’V
k
	â€‹

) â‡’ modest 
ğ¼
prac
I
prac
 â‡’ practice only if costs are tiny or discounting light.

Hard to improve & low payoff: small 
ğœ‚
ğ‘˜
Î·
k
	â€‹

 and small gap â‡’ tiny 
ğ¼
prac
I
prac
 â‡’ donâ€™t practice; just use something else.

APs vs MA fit naturally:

APs typically have low 
ğ‘
use
c
use
, often small remaining gap (already close to 
ğ‘‰
max
â¡
V
max
); practicing them only wins if a big payoff remains (promotion to expert-level).

MA routines have higher 
ğ‘
prac
c
prac
/
ğ‘
use
c
use
 but can have enormous 
ğ‘‰
max
â¡
V
max
; when 
ğœ‚
ğ‘˜
Î·
k
	â€‹

 is high (youâ€™re in a learning-rich regime) and discounting is gentle, investing in MA wins.

Short answer: youâ€™re not inventing a new metric. Your

ğ‘ˆ
ğ‘–
(
ğ‘¡
)
â€…â€Š
=
â€…â€Š
âˆ‘
ğœ
=
0
ğ»
(
ğ‘¡
)
ğ›¾
(
ğ‘¡
)
ğœ
â€…â€Š
ğ¸
â€‰â£
[
Î”
F
E
ğ‘–
,
ğ‘¡
+
ğœ
]
ğ¶
ğ‘–
(
ğ‘¡
)
U
i
	â€‹

(t)=
C
i
	â€‹

(t)
Ï„=0
âˆ‘
H(t)
	â€‹

Î³(t)
Ï„
E[Î”FE
i,t+Ï„
	â€‹

]
	â€‹


is just a tidy remix of pieces that are already standard:

How each piece maps to established results

â€œDiscounted stream of valueâ€ (numerator).
In active inference, policies are chosen by minimizing expected free energy over a finite horizon; the textbook expression is a discounted sum across future time steps (risk + ambiguity + epistemic terms). That is exactly your 
âˆ‘
ğœ
ğ›¾
ğœ
ğ¸
[
Î”
F
E
]
âˆ‘
Ï„
	â€‹

Î³
Ï„
E[Î”FE]. 
Chris Mathys
ScienceDirect
PMC

â€œPer unit costâ€ (denominator).
Two well-established traditions encode cost to act/compute:

Opportunity-cost / reward-rate RL: tonic dopamine tracks the average reward rate, i.e., the price of time, and sets vigor accordinglyâ€”an explicit value-per-time (or energy) calculus. 
PubMed
Princeton University
Europe PMC

Effort cost in control allocation (EVC): dACC selects control intensity by trading expected payoff against effort cost; the decision variable is literally â€œbenefits vs costs of control.â€ 
PMC
PubMed

Empirically, dopamine shifts the benefitâ†”cost balance when people decide whether to expend cognitive effortâ€”again, a benefit-per-cost computation. 
PubMed
Science
PMC

â€œOne currency across disparate goals.â€
Human vmPFC/ventral striatum encode a common neural currency for subjective value across very different goods and costs (money, goods, social, and effort). This is the standard way the field operationalizes comparability of apples vs oranges. 
PubMed
+1
PMC

vmPFC also carries intertemporal subjective value during delay-discountingâ€”i.e., net present value in the brain. 
PubMed
PMC

The same valuation network jointly encodes benefits and cognitive-effort costs. 
jneurosci.org
+1

â€œArbiter picks the winner.â€
There is a neural arbiter: dACC (EVC) integrates benefits and costs of control; BG gates the chosen policy; and arbitration between MB/MF has been shown to track system reliabilities (a special case of expected future loss reduction). 
PMC
+1

â€œRate/efficiency selection is old biology.â€
In ecology, animals maximize gain per unit cost/time (optimal foraging; marginal value theorem). Thatâ€™s the same ratio form your 
ğ‘ˆ
ğ‘–
U
i
	â€‹

 uses. 
paulseabright.com
Gwern

â€œBounded rationality as free-energy / information trade-off.â€
In control and decision theory, KL-control / linearly-solvable MDPs and information-theoretic bounded rationality pick policies by maximizing expected utility minus a resource cost, i.e., a free-energy-like functionalâ€”another articulation of â€œvalue per cost.â€ 
papers.neurips.cc
PNAS
arXiv

â€œCosts are real/metabolic.â€
Neural signaling is expensive; cortical computation is energy-limited. The biophysics supports treating cost 
ğ¶
ğ‘–
C
i
	â€‹

 as something the system must price. 
PubMed
+1

Ratio vs. difference (why your 
ğµ
/
ğ¶
B/C matches classic â€œbenefit â€“ Î»Â·costâ€)

Many frameworks maximize benefit â€“ Î»Â·cost rather than a ratio. Under a resource (time/energy) constraint, the two are equivalent at the optimum: choosing controller 
ğ‘–
i by largest 
ğµ
ğ‘–
/
ğ¶
ğ‘–
B
i
	â€‹

/C
i
	â€‹

 is identical to choosing by 
ğµ
ğ‘–
âˆ’
ğœ†
ğ¶
ğ‘–
B
i
	â€‹

âˆ’Î»C
i
	â€‹

 for some shadow price 
ğœ†
Î» (the current opportunity cost of time/energy). In the brain, tonic DA â‰ˆ 
ğœ†
Î» (average reward rate), which converts the ratio into the familiar difference form used by dACC/EVC. Empirically, manipulating DA or the opportunity cost of time tilts choices exactly as that duality predicts. 
PubMed
bioRxiv
PMC

Minimal biological wiring diagram

Computation of the numerator (discounted value): vmPFC/VS encode subjective (discounted) value; hippocampus/PFC supply model-based predictions (where applicable); active-inference accounts formalize this as expected free energy over horizon 
ğ»
(
ğ‘¡
)
H(t). 
PubMed
+1
Chris Mathys

Computation of the denominator (cost): dACC/EVC and striatum integrate effort/time costs; DA implements the opportunity-cost price; NE (LC) sets adaptive gain/precision (exploreâ€“exploit), changing effective costs/benefits. 
PMC
PubMed
+1

Selection/gating: dACC signals control demand; BG and STN gate the chosen policy; when deliberation time has a cost, behavior follows standard bounded-accumulation with explicit time-penalties. 
PMC
+1
jneurosci.org

Bottom line

The form â€œdiscounted future goodness over present costâ€ is not novel: itâ€™s the intersection of active inferenceâ€™s expected free energy, reward-rate/effort accounts (DA, EVC), optimal foragingâ€™s rate maximization, and information-theoretic bounded rationality.

What is yours is the specific packaging: using 
Î”
F
E
Î”FE as the benefit signal across controllers and making the ratio explicit. But every â€œLEGO brickâ€ of that package already existsâ€”and has neural support.

Unified Predictive Control Theory (IACE â‰ˆ UPCA)
1. Core Claim

Biological control is a resource-priced arbitration between cheap, habitual policies and costly, generative simulation. The brain escalates to model-based control only when residual error persists and resources permit, with admission to global broadcast gated by identifiable thalamo-basal ganglia circuits. This yields ordered degradation under scarcity, oscillations under conflict, and specific neuromodulator Ã— receptor Ã— circuit effects.

2. Surplus Signal (SÌ‚): Inference, Not Homunculus
2.1 Latent Allostatic State

Surplus is not a single â€œfuel gauge.â€ Instead, the brain infers a low-dimensional latent state Sâƒ— from heterogeneous interoceptive/autonomic signals:

ğ‘¥
(
ğ‘¡
)
=
[
ğ¸
fast


ğ¸
mid


ğ¸
slow


ğ‘‚
2


ğ¶
ğ‘‚
2


ğ»
sleep


ğ»
hydr
]
,
ğ‘¥
Ë™
=
ğ´
ğ‘¥
+
ğ‘¢
(
ğ‘¡
)
+
ğœ‚
(
ğ‘¡
)
x(t)=
	â€‹

E
fast
	â€‹

E
mid
	â€‹

E
slow
	â€‹

O
2
	â€‹

CO
2
	â€‹

H
sleep
	â€‹

H
hydr
	â€‹

	â€‹

	â€‹

,
x
Ë™
=Ax+u(t)+Î·(t)

E_fast: ATP/Oâ‚‚, secondsâ€“minutes

E_mid: glucose/glycogen, minutesâ€“hours

E_slow: fatigue/recovery, hoursâ€“days

H_sleep: homeostatic sleep drive (Process-S)

H_hydr: hydration/osmotic balance

Oâ‚‚, COâ‚‚: gas exchange constraints

Sensors (noisy): HR/HRV, pupil, end-tidal COâ‚‚, SpOâ‚‚, glucose, skin conductance, fNIRS/fMRI BOLD.

Discrete inference:

ğ‘¥
^
ğ‘¡
âˆ£
ğ‘¡
âˆ’
1
=
ğ¹
ğ‘¥
^
ğ‘¡
âˆ’
1
+
ğµ
ğ‘¢
ğ‘¡
âˆ’
1
,
ğ¾
ğ‘¡
=
ğ‘ƒ
ğ‘¡
âˆ£
ğ‘¡
âˆ’
1
ğ¶
âŠ¤
(
ğ¶
ğ‘ƒ
ğ‘¡
âˆ£
ğ‘¡
âˆ’
1
ğ¶
âŠ¤
+
ğ‘…
)
âˆ’
1
x
^
tâˆ£tâˆ’1
	â€‹

=F
x
^
tâˆ’1
	â€‹

+Bu
tâˆ’1
	â€‹

,K
t
	â€‹

=P
tâˆ£tâˆ’1
	â€‹

C
âŠ¤
(CP
tâˆ£tâˆ’1
	â€‹

C
âŠ¤
+R)
âˆ’1
ğ‘¥
^
ğ‘¡
=
ğ‘¥
^
ğ‘¡
âˆ£
ğ‘¡
âˆ’
1
+
ğ¾
ğ‘¡
(
ğ‘¦
ğ‘¡
âˆ’
ğ¶
ğ‘¥
^
ğ‘¡
âˆ£
ğ‘¡
âˆ’
1
)
,
ğ‘†
^
ğ‘¡
=
softplus
(
ğ‘¤
âŠ¤
ğ‘¥
^
ğ‘¡
+
ğ‘
)
x
^
t
	â€‹

=
x
^
tâˆ£tâˆ’1
	â€‹

+K
t
	â€‹

(y
t
	â€‹

âˆ’C
x
^
tâˆ£tâˆ’1
	â€‹

),
S
^
t
	â€‹

=softplus(w
âŠ¤
x
^
t
	â€‹

+b)

with monotone 
ğ‘¤
â‰¥
0
wâ‰¥0.

2.2 Neuroanatomical Substrates

Sensing: NTS, parabrachial nucleus, carotid body, vagal afferents

Integration: posterior/anterior insula, vmPFC (predictive coding hub); hypothalamus (slow drives)

Broadcast: insula/vmPFC â†’ ACC/FPC; hypothalamus â†’ LC/VTA/NBM for global gain

Key property: SÌ‚ is a fit-and-test latent. If it fails to track behavior under metabolic clamps (hypoxia, ketone infusion, glucose), the model fails.

3. Threshold (Î¸): Circuit-Level Gating Bound

Î¸ is not a scalar knob but a composite of identifiable gating mechanisms:

ğœƒ
ğ‘¡
=
ğœƒ
0
+
ğ‘˜
TRN
ğ‘”
TRN
(
ğ‘¡
)
+
ğ‘˜
STN
ğ‘Ÿ
STN
(
ğ‘¡
)
âˆ’
ğ‘˜
ğ·
1
ğ‘
ğ·
1
(
ğ‘¡
)
+
ğ‘˜
ğ·
2
ğ‘
ğ·
2
(
ğ‘¡
)
+
ğ‘˜
FPC
ğ‘
FPC
(
ğ‘¡
)
Î¸
t
	â€‹

=Î¸
0
	â€‹

+k
TRN
	â€‹

g
TRN
	â€‹

(t)+k
STN
	â€‹

r
STN
	â€‹

(t)âˆ’k
D1
	â€‹

a
D1
	â€‹

(t)+k
D2
	â€‹

a
D2
	â€‹

(t)+k
FPC
	â€‹

b
FPC
	â€‹

(t)

TRN: thalamic reticular nucleus â†’ broadcast gate

STN: hyperdirect pathway â†’ â€œhold your horsesâ€ brake

D1/D2 striatum: Go/No-Go bias

FPC: raises threshold when multiple alternatives active

Operationalization:

Î¸_str (BG update gate) = striatal Go/NoGo balance; measured via PBWM/response-threshold DDM

Î¸_brd (broadcast ignition) = TRN/parietal-PFC ignition; measured via P3b amplitude, PLV synchrony, TMSâ€“EEG effective connectivity

ğœƒ
=
max
â¡
(
ğœƒ
str
,
ğœƒ
brd
)
Î¸=max(Î¸
str
	â€‹

,Î¸
brd
	â€‹

)

4. Arbitration: Market of Competing Controllers

Controllers bid for control:

MF habits: dorsolateral striatum, cerebellum

MB planning: dorsomedial striatum + PFC/hippocampus

Pavlovian reflex: amygdala/brainstem

Interoceptive homeostasis: insula/hypothalamus

Exploration/branching: FPC

Auction dynamics:

ğ‘§
Ë™
ğ‘–
=
ğ›½
ğ‘–
(
ğ‘†
^
)
ğ‘ˆ
ğ‘–
âˆ’
ğœ†
ğ‘§
ğ‘–
âˆ’
âˆ‘
ğ‘—
â‰ 
ğ‘–
ğ›¾
ğ‘–
ğ‘—
ğ‘§
ğ‘—
+
ğœ‰
ğ‘–
z
Ë™
i
	â€‹

=Î²
i
	â€‹

(
S
^
)U
i
	â€‹

âˆ’Î»z
i
	â€‹

âˆ’
j
î€ 
=i
âˆ‘
	â€‹

Î³
ij
	â€‹

z
j
	â€‹

+Î¾
i
	â€‹

ğ‘ˆ
ğ‘–
=
ğ¸
[
Î”
ğ¹
ğ¸
ğ‘–
]
time_cost
ğ‘–
(
DA,Â 5-HT
)
+
energy_cost
ğ‘–
(
ğ‘†
^
)
U
i
	â€‹

=
time_cost
i
	â€‹

(DA,Â 5-HT)+energy_cost
i
	â€‹

(
S
^
)
E[Î”FE
i
	â€‹

]
	â€‹


Gate opens for controller i when 
ğ‘§
ğ‘–
>
ğœƒ
ğ‘¡
z
i
	â€‹

>Î¸
t
	â€‹

.

Oscillations

Conflict yields oscillations if cross-inhibition + fatigue/delays induce limit cycles (e.g., all-nighter toggling between â€œpush workâ€ and â€œfall asleepâ€). Predictable Hopf regime: 
ğ›¾
>
ğœ†
,
ğœ…
>
0
,
ğœ
ğ‘‘
>
0
Î³>Î»,Îº>0,Ï„
d
	â€‹

>0.

5. Neuromodulator Matrix

Not knobs, but receptor Ã— target Ã— timescale effects:

Tx	Receptor	Target	Effect
NE (LC)	Î±1/Î²	PFC gain â†‘, TRN â†“, FPC exploration â†‘	Lowers Î¸ at moderate levels; too high = reset
NE (LC)	Î±2	PFC recurrent stabilizer â†‘	Raises Î¸ in fatigue
ACh (nAChR)	thalamus/PFC relay	Input gain â†‘, encoding â†‘	Lowers bound for new evidence
ACh (mAChR, M1/M2)	PFC recurrent, TRN	Stabilization â†‘	Raises bound; REM protection
DA (D1)	striatum direct	Go bias â†‘, policy precision â†‘	Shortens deliberation; effective Î¸â†‘ under time pressure
DA (D2)	striatum indirect	No-go bias â†‘	Raises Î¸; cautious
5-HT	1A/2A	PFC integration horizon	Raises Î¸; patience â†‘; 2A state-dependent lowering in psychedelics
6. Learning & Plasticity

Weights update by:

Î”
ğ‘¤
âˆ
Hebb
Ã—
ğ‘”
ctrl
Ã—
predictionÂ error
Î£
Ã—
salience
âˆ’
downscale
Î”wâˆHebbÃ—g
ctrl
	â€‹

Ã—
Î£
predictionÂ error
	â€‹

Ã—salienceâˆ’downscale

Controller tag: plasticity proportional to which controller won

Sleep:

NREM: hippocampal replay â†’ MB credit assignment

REM: muscarinic stabilization + recombination; pruning of unused traces

7. Predictions & Falsifiability

Operational readouts

SÌ‚: latent filter fit to physiological channels; validated against clamps

Î¸: separate Î¸_str (BG gating indices) and Î¸_brd (ignition markers)

Directional tests

Hypoxia/COâ‚‚: lowers Oâ‚‚ latent â†’ SÌ‚â†“ â†’ reduced MB weight despite constant Îµ_res

Insula TMS: noisy SÌ‚ (posterior variance â†‘) â†’ more LC-NE resets; less stable MB engagement

nAChR vs mAChR: nicotine lowers Î¸, scopolamine raises Î¸; if both same, reject receptor-specificity

STN-DBS: reduces Î¸_str (faster commit) but not MB admission; if MB admission â†‘, Î¸ mislocalized

DA/urgency: raises opportunity cost; paradoxically reduces MB even with high SÌ‚

Sleep pressure: MB fails first, then executive, then habit â†’ reproduces clinical hypoglycemia/hypoxia ordering

Ketone infusion: increases MB if SÌ‚ tracks true energy, not just glucose

Pre-registered nulls: if â‰¥2 fail, theory is falsified (not â€œsavedâ€ post hoc).

8. Integration with IACE / UPCA

Mapping:

IACE	UPCA	Circuit	Observable
Speculative Engine (MA)	MB planner	PFCâ€“hipp	replay, Î”FE estimate
Automatic Pathways (APs)	SI/habits	DLS, cerebellum	MF weight
Subconscious Allocator (SC)	AMC	ACC/FPC â†” BG gate	Î¸_str, Î¸_brd
Surplus	SÌ‚ filter	Insula/hypothalamus/brainstem	S latents
Global broadcast	Ignition gate	TRN + parietalâ€“PFC	P3b/PLV/TMS-EEG

Thus IACE and UPCA are the same control principle at different descriptions.

9. Compact Formulas

MB engagement:

ğ‘”
MB
=
ğœ
(
ğ›¼
ğœ€
res
+
ğ›½
ğ‘†
^
âˆ’
ğœƒ
ğ‘¡
)
g
MB
	â€‹

=Ïƒ(Î±Îµ
res
	â€‹

+Î²
S
^
âˆ’Î¸
t
	â€‹

)

Gate bound:

ğœƒ
ğ‘¡
=
ğœƒ
(
ğ‘‡
ğ‘…
ğ‘
,
ğ‘†
ğ‘‡
ğ‘
,
ğ·
1
/
ğ·
2
,
ğ¹
ğ‘ƒ
ğ¶
;
Â 
ğ‘
ğ¸
,
ğ´
ğ¶
â„
ğ‘›
/
ğ‘š
,
ğ·
ğ´
,
5
âˆ’
ğ»
ğ‘‡
)
Î¸
t
	â€‹

=Î¸(TRN,STN,D1/D2,FPC;Â NE,ACh
n/m
	â€‹

,DA,5âˆ’HT)

Auction dynamics:

ğ‘§
Ë™
ğ‘–
=
ğ›½
ğ‘–
(
ğ‘†
^
)
ğ‘ˆ
ğ‘–
âˆ’
ğœ†
ğ‘§
ğ‘–
âˆ’
âˆ‘
ğ‘—
â‰ 
ğ‘–
ğ›¾
ğ‘–
ğ‘—
ğ‘§
ğ‘—
+
ğœ‰
ğ‘–
z
Ë™
i
	â€‹

=Î²
i
	â€‹

(
S
^
)U
i
	â€‹

âˆ’Î»z
i
	â€‹

âˆ’
j
î€ 
=i
âˆ‘
	â€‹

Î³
ij
	â€‹

z
j
	â€‹

+Î¾
i
	â€‹

10. Why It Matters

Explains: imagery biases, intuition, sleep-dependent pruning, hypoglycemia/hypoxia collapse ordering, scarcity-graded control degradation

Novel predictions: ACh double dissociation (nAChR vs mAChR), DA urgency paradox, ketone infusion effect

Implementable: State-space SÌ‚ filter, auction gate, plasticity update = runnable code

Falsifiable: pre-registered manipulations with declared fail criteria
